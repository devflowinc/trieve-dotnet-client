/*
 * Trieve API
 *
 * Trieve OpenAPI Specification. This document describes all of the operations available through the Trieve API.
 *
 * The version of the OpenAPI document: 0.5.0
 * Contact: developers@trieve.ai
 * Generated by: https://github.com/openapitools/openapi-generator.git
 */


using System;
using System.Collections;
using System.Collections.Generic;
using System.Collections.ObjectModel;
using System.Linq;
using System.IO;
using System.Runtime.Serialization;
using System.Text;
using System.Text.RegularExpressions;
using Newtonsoft.Json;
using Newtonsoft.Json.Converters;
using Newtonsoft.Json.Linq;
using System.ComponentModel.DataAnnotations;
using OpenAPIDateConverter = trieve_dotnet_client.Client.OpenAPIDateConverter;

namespace trieve_dotnet_client.Model
{
    /// <summary>
    /// GenerateChunksRequest
    /// </summary>
    [DataContract(Name = "GenerateChunksRequest")]
    public partial class GenerateChunksRequest : IValidatableObject
    {
        /// <summary>
        /// Initializes a new instance of the <see cref="GenerateChunksRequest" /> class.
        /// </summary>
        [JsonConstructorAttribute]
        protected GenerateChunksRequest() { }
        /// <summary>
        /// Initializes a new instance of the <see cref="GenerateChunksRequest" /> class.
        /// </summary>
        /// <param name="chunkIds">The ids of the chunks to be retrieved and injected into the context window for RAG. (required).</param>
        /// <param name="model">The model to use for the chat. This can be any model from the openrouter model list. If no model is provided, gpt-3.5-turbo will be used..</param>
        /// <param name="prevMessages">The previous messages to be placed into the chat history. The last message in this array will be the prompt for the model to inference on. The length of this array must be at least 1. (required).</param>
        /// <param name="prompt">Prompt for the last message in the prev_messages array. This will be used to generate the next message in the chat. The default is &#39;Respond to the instruction and include the doc numbers that you used in square brackets at the end of the sentences that you used the docs for:&#39;. You can also specify an empty string to leave the final message alone such that your user&#39;s final message can be used as the prompt. See docs.trieve.ai or contact us for more information..</param>
        /// <param name="streamResponse">Whether or not to stream the response. If this is set to true or not included, the response will be a stream. If this is set to false, the response will be a normal JSON response. Default is true..</param>
        public GenerateChunksRequest(List<Guid> chunkIds = default(List<Guid>), string model = default(string), List<ChatMessageProxy> prevMessages = default(List<ChatMessageProxy>), string prompt = default(string), bool? streamResponse = default(bool?))
        {
            // to ensure "chunkIds" is required (not null)
            if (chunkIds == null)
            {
                throw new ArgumentNullException("chunkIds is a required property for GenerateChunksRequest and cannot be null");
            }
            this.ChunkIds = chunkIds;
            // to ensure "prevMessages" is required (not null)
            if (prevMessages == null)
            {
                throw new ArgumentNullException("prevMessages is a required property for GenerateChunksRequest and cannot be null");
            }
            this.PrevMessages = prevMessages;
            this.Model = model;
            this.Prompt = prompt;
            this.StreamResponse = streamResponse;
        }

        /// <summary>
        /// The ids of the chunks to be retrieved and injected into the context window for RAG.
        /// </summary>
        /// <value>The ids of the chunks to be retrieved and injected into the context window for RAG.</value>
        [DataMember(Name = "chunk_ids", IsRequired = true, EmitDefaultValue = true)]
        public List<Guid> ChunkIds { get; set; }

        /// <summary>
        /// The model to use for the chat. This can be any model from the openrouter model list. If no model is provided, gpt-3.5-turbo will be used.
        /// </summary>
        /// <value>The model to use for the chat. This can be any model from the openrouter model list. If no model is provided, gpt-3.5-turbo will be used.</value>
        [DataMember(Name = "model", EmitDefaultValue = true)]
        public string Model { get; set; }

        /// <summary>
        /// The previous messages to be placed into the chat history. The last message in this array will be the prompt for the model to inference on. The length of this array must be at least 1.
        /// </summary>
        /// <value>The previous messages to be placed into the chat history. The last message in this array will be the prompt for the model to inference on. The length of this array must be at least 1.</value>
        [DataMember(Name = "prev_messages", IsRequired = true, EmitDefaultValue = true)]
        public List<ChatMessageProxy> PrevMessages { get; set; }

        /// <summary>
        /// Prompt for the last message in the prev_messages array. This will be used to generate the next message in the chat. The default is &#39;Respond to the instruction and include the doc numbers that you used in square brackets at the end of the sentences that you used the docs for:&#39;. You can also specify an empty string to leave the final message alone such that your user&#39;s final message can be used as the prompt. See docs.trieve.ai or contact us for more information.
        /// </summary>
        /// <value>Prompt for the last message in the prev_messages array. This will be used to generate the next message in the chat. The default is &#39;Respond to the instruction and include the doc numbers that you used in square brackets at the end of the sentences that you used the docs for:&#39;. You can also specify an empty string to leave the final message alone such that your user&#39;s final message can be used as the prompt. See docs.trieve.ai or contact us for more information.</value>
        [DataMember(Name = "prompt", EmitDefaultValue = true)]
        public string Prompt { get; set; }

        /// <summary>
        /// Whether or not to stream the response. If this is set to true or not included, the response will be a stream. If this is set to false, the response will be a normal JSON response. Default is true.
        /// </summary>
        /// <value>Whether or not to stream the response. If this is set to true or not included, the response will be a stream. If this is set to false, the response will be a normal JSON response. Default is true.</value>
        [DataMember(Name = "stream_response", EmitDefaultValue = true)]
        public bool? StreamResponse { get; set; }

        /// <summary>
        /// Returns the string presentation of the object
        /// </summary>
        /// <returns>String presentation of the object</returns>
        public override string ToString()
        {
            StringBuilder sb = new StringBuilder();
            sb.Append("class GenerateChunksRequest {\n");
            sb.Append("  ChunkIds: ").Append(ChunkIds).Append("\n");
            sb.Append("  Model: ").Append(Model).Append("\n");
            sb.Append("  PrevMessages: ").Append(PrevMessages).Append("\n");
            sb.Append("  Prompt: ").Append(Prompt).Append("\n");
            sb.Append("  StreamResponse: ").Append(StreamResponse).Append("\n");
            sb.Append("}\n");
            return sb.ToString();
        }

        /// <summary>
        /// Returns the JSON string presentation of the object
        /// </summary>
        /// <returns>JSON string presentation of the object</returns>
        public virtual string ToJson()
        {
            return Newtonsoft.Json.JsonConvert.SerializeObject(this, Newtonsoft.Json.Formatting.Indented);
        }

        /// <summary>
        /// To validate all properties of the instance
        /// </summary>
        /// <param name="validationContext">Validation context</param>
        /// <returns>Validation Result</returns>
        IEnumerable<System.ComponentModel.DataAnnotations.ValidationResult> IValidatableObject.Validate(ValidationContext validationContext)
        {
            yield break;
        }
    }

}
